W0811 12:05:05.617000 139709738891072 torch/distributed/run.py:779] 
W0811 12:05:05.617000 139709738891072 torch/distributed/run.py:779] *****************************************
W0811 12:05:05.617000 139709738891072 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0811 12:05:05.617000 139709738891072 torch/distributed/run.py:779] *****************************************
08/11/2024 12:05:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
08/11/2024 12:05:10 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[INFO|tokenization_utils_base.py:2106] 2024-08-11 12:05:10,652 >> loading file tokenizer.model
[INFO|tokenization_utils_base.py:2106] 2024-08-11 12:05:10,652 >> loading file added_tokens.json
[INFO|tokenization_utils_base.py:2106] 2024-08-11 12:05:10,652 >> loading file special_tokens_map.json
[INFO|tokenization_utils_base.py:2106] 2024-08-11 12:05:10,652 >> loading file tokenizer_config.json
[INFO|tokenization_utils_base.py:2106] 2024-08-11 12:05:10,652 >> loading file tokenizer.json
08/11/2024 12:05:10 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.
08/11/2024 12:05:10 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16
[WARNING|logging.py:314] 2024-08-11 12:05:11,111 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
08/11/2024 12:05:11 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
08/11/2024 12:05:11 - INFO - llamafactory.data.loader - Loading dataset CHEF_DPO_Train_data.json...
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
08/11/2024 12:05:11 - INFO - llamafactory.data.template - Add <|user|>,<|observation|> to stop words.
Converting format of dataset (num_proc=16): 100%|█████████| 700/700 [00:00<00:00, 3241.40 examples/s]
08/11/2024 12:05:12 - INFO - llamafactory.data.loader - Loading dataset CHEF_DPO_Train_data.json...
Running tokenizer on dataset (num_proc=16): 100%|███████████| 700/700 [00:14<00:00, 47.65 examples/s]
training example:
chosen_input_ids:
[151331, 151333, 151336, 198, 98406, 99950, 100867, 111434, 99788, 100774, 120950, 100153, 3837, 106300, 99245, 102243, 98327, 117827, 103703, 102676, 100774, 1773, 698, 1, 113440, 99089, 100968, 103189, 5373, 102676, 102243, 99073, 100968, 103189, 117827, 3837, 103517, 99245, 102243, 98327, 117827, 3837, 103703, 101688, 100774, 3837, 100205, 100968, 103189, 99921, 125900, 14, 113068, 14, 102243, 100767, 109659, 110122, 1773, 698, 1, 99654, 3837, 99444, 100774, 98339, 111830, 102243, 98327, 117827, 98341, 1773, 698, 1, 100968, 103189, 98323, 5122, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 98315, 125319, 98575, 698, 1, 102917, 102243, 98323, 5122, 1183, 102243, 16, 25, 10231, 114, 110, 119851, 12874, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 109229, 125319, 98575, 3837, 25131, 118, 98322, 101067, 98483, 105954, 98380, 101839, 10424, 98314, 103973, 3837, 110653, 98790, 90289, 231, 3837, 98596, 119851, 109254, 98413, 113427, 98316, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 114446, 125931, 127255, 98468, 106644, 3837, 101839, 24090, 250, 98544, 98314, 109254, 3837, 107912, 98596, 119851, 115447, 98658, 105764, 107388, 98356, 98670, 122055, 1773, 98924, 98596, 119851, 106679, 98847, 103973, 119394, 98319, 113490, 99381, 98596, 119851, 5122, 98790, 90289, 231, 98539, 13084, 233, 5122, 113427, 131599, 102866, 120347, 119851, 115447, 98658, 98322, 99627, 99184, 53389, 131359, 124001, 3837, 98516, 108808, 111307, 49103, 110, 103973, 98413, 113427, 98316, 107388, 98356, 126309, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 127255, 100150, 101839, 109254, 3837, 107912, 98670, 122055, 98314, 101839, 12268, 246, 112237, 1773, 100470, 3837, 100785, 98319, 120347, 102335, 100813, 99148, 134479, 99362, 103294, 105954, 24090, 250, 98544, 101839, 127255, 98468, 1773, 98634, 105189, 98596, 119851, 115447, 98658, 98415, 99627, 99184, 53389, 103973, 3837, 102047, 98378, 105954, 127255, 98468, 105448, 64744, 50, 963, 27916, 3591, 101339, 98314, 114446, 125931, 98828, 249, 121338, 127255, 98468, 26852, 2304, 21755, 39930, 97371, 546, 1296, 372, 17211, 24991, 105189, 98546, 16, 98334, 106644, 108808, 98771, 98314, 109254, 3837, 98396, 101457, 103973, 100051, 103577, 144770, 121288, 102474, 107778, 12693, 704, 69, 3430, 23584, 594, 34, 2437, 355, 6691, 66, 22679, 774, 531, 70000, 94967, 70, 11170, 97371, 11297, 107498, 1773, 98836, 99099, 248, 105367, 116184, 28048, 232, 24273, 226, 98314, 138548, 99029, 113395, 98478, 3837, 106679, 102047, 107388, 98356, 121828, 144140, 7536, 110, 12268, 246, 98415, 13503, 114, 107388, 98314, 24090, 250, 98544, 101839, 3837, 119394, 98686, 120347, 119851, 105764, 98316, 12874, 25131, 118, 116718, 101067, 98483, 105954, 98380, 10424, 98314, 102949, 1773, 100889, 3837, 99765, 110511, 113911, 165, 235, 113, 98845, 100750, 15786, 233, 3837, 102057, 100813, 107388, 98356, 126309, 98833, 121281, 24090, 250, 98544, 101839, 12874, 98324, 125343, 98320, 107029, 99281, 3837, 98836, 99099, 248, 98790, 90289, 231, 3837, 120347, 119851, 100395, 98316, 89928, 107, 44557, 97, 98314, 1773, 98389, 25131, 101, 139619, 98320, 98837, 114811, 110511, 98647, 3837, 98316, 98578, 98443, 123152, 102281, 105150, 99508, 3837, 98317, 14665, 101, 99220, 109886, 98316, 99162, 101044, 107711, 98759, 117283, 3837, 101015, 99203, 115, 136584, 110511, 99002, 106454, 98421, 147267, 98470, 135700, 1773, 5890, 5, 9880, 64598, 50, 49285, 89086, 11, 49495, 64654, 1958, 44, 1568, 50, 49285, 25134, 54385, 5, 9880, 28154, 58437, 7039, 0, 98948, 110042, 114143, 121, 98316, 121924, 111817, 111295, 3837, 99332, 98813, 107388, 107507, 98743, 98401, 108808, 111307, 3837, 98948, 100241, 98819, 100583, 98384, 124834, 108081, 105189, 110604, 3837, 98833, 13503, 114, 138813, 99526, 162, 3219, 162, 3219, 98384, 102823, 6313, 112259, 100049, 118156, 98603, 143033, 3837, 145642, 116184, 98617, 13837, 107, 98671, 3837, 133841, 130501, 98501, 98355, 251, 98502, 148800, 119594, 98433, 3837, 36504, 101, 99423, 100038, 117695, 133545, 98366, 15201, 231, 98344, 133545, 98332, 3837, 98319, 120347, 98550, 114446, 98650, 104111, 103178, 98314, 98350, 112944, 3837, 112619, 102626, 13, 17, 4, 98453, 105738, 112944, 123456, 3837, 98949, 99835, 98429, 125931, 25131, 231, 98416, 98502, 148800, 102191, 98481, 16616, 228, 98991, 110614, 98347, 111307, 98911, 235, 44467, 252, 98314, 126665, 3837, 98896, 105656, 98318, 101400, 103514, 136713, 108808, 98330, 3837, 126666, 98908, 102191, 98314, 115447, 98416, 98361, 111234, 98788, 71941, 110, 98442, 3837, 98603, 143033, 98344, 109224, 98862, 102785, 99007, 141509, 3838, 2982, 477, 383, 26431, 1055, 2145, 41350, 70, 5372, 7201, 4803, 30, 9880, 411, 20065, 64, 27669, 83, 49508, 69, 43, 1370, 2584, 3966, 10666, 137505, 18779, 941, 89064, 1087, 1958, 1778, 437, 11888, 4142, 541, 258, 603, 0, 98317, 118421, 98316, 111234, 105735, 99090, 5373, 109652, 13522, 97, 5373, 102821, 142152, 100332, 99090, 3837, 105904, 101005, 101094, 114143, 121, 105656, 119594, 98330, 28048, 109, 98362, 98882, 98680, 247, 3837, 99533, 118221, 102156, 124834, 98367, 3837, 103636, 105656, 106695, 103396, 108, 99109, 98810, 28048, 112, 3837, 115193, 110511, 100499, 98882, 98680, 247, 109470, 98983, 98542, 112237, 99370, 98361, 105656, 99896, 98603, 143033, 9904, 100149, 98551, 124553, 98412, 48852, 247, 20965, 250, 105304, 105288, 99869, 98385, 114446, 7552, 105738, 98321, 106660, 98551, 98663, 104557, 5373, 98485, 2198, 102243, 17, 25, 10231, 114, 110, 119851, 12874, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 109229, 125319, 98575, 3837, 25131, 118, 98322, 101067, 98483, 105954, 98380, 101839, 10424, 98314, 103973, 3837, 110653, 98790, 90289, 231, 3837, 98596, 119851, 109254, 98413, 113427, 98316, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 114446, 125931, 127255, 98468, 106644, 3837, 101839, 24090, 250, 98544, 98314, 109254, 3837, 107912, 98596, 119851, 115447, 98658, 105764, 107388, 98356, 98670, 122055, 1773, 98924, 98596, 119851, 106679, 98847, 103973, 119394, 98319, 113490, 99381, 98596, 119851, 5122, 98790, 90289, 231, 98539, 13084, 233, 5122, 113427, 131599, 102866, 120347, 119851, 115447, 98658, 98322, 99627, 99184, 53389, 131359, 124001, 3837, 98516, 108808, 111307, 49103, 110, 103973, 98413, 113427, 98316, 107388, 98356, 126309, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 127255, 100150, 101839, 109254, 3837, 107912, 98670, 122055, 98314, 101839, 12268, 246, 112237, 1773, 100470, 3837, 100785, 98319, 120347, 102335, 100813, 99148, 134479, 99362, 103294, 105954, 24090, 250, 98544, 101839, 127255, 98468, 1773, 98634, 105189, 98596, 119851, 115447, 98658, 98415, 99627, 99184, 53389, 103973, 3837, 102047, 98378, 105954, 127255, 98468, 105448, 64744, 50, 963, 27916, 3591, 101339, 98314, 114446, 125931, 98828, 249, 121338, 127255, 98468, 26852, 2304, 21755, 39930, 97371, 546, 1296, 372, 17211, 24991, 105189, 98546, 16, 98334, 106644, 108808, 98771, 98314, 109254, 3837, 98396, 101457, 103973, 100051, 103577, 144770, 121288, 102474, 107778, 12693, 704, 69, 3430, 23584, 594, 34, 2437, 355, 6691, 66, 22679, 774, 531, 70000, 94967, 70, 11170, 97371, 11297, 107498, 1773, 98836, 99099, 248, 105367, 116184, 28048, 232, 24273, 226, 98314, 138548, 99029, 113395, 98478, 3837, 106679, 102047, 107388, 98356, 121828, 144140, 7536, 110, 12268, 246, 98415, 13503, 114, 107388, 98314, 24090, 250, 98544, 101839, 3837, 119394, 98686, 120347, 119851, 105764, 98316, 12874, 25131, 118, 116718, 101067, 98483, 105954, 98380, 10424, 98314, 102949, 1773, 100889, 3837, 99765, 110511, 113911, 165, 235, 113, 98845, 100750, 15786, 233, 3837, 102057, 100813, 107388, 98356, 126309, 98833, 121281, 24090, 250, 98544, 101839, 12874, 98324, 125343, 98320, 107029, 99281, 3837, 98836, 99099, 248, 98790, 90289, 231, 3837, 120347, 119851, 100395, 98316, 89928, 107, 44557, 97, 98314, 1773, 124553, 98412, 29653, 225, 102018, 11615, 229, 94651, 126544, 122269, 126666, 99007, 98492, 99296, 98324, 105556, 118117, 99599, 98473, 100053, 99211, 98385, 61570, 231, 15815, 99, 126247, 22701, 228, 38887, 95, 110318, 100652, 98447, 3837, 98333, 12874, 24090, 247, 35457, 110, 7536, 110, 98585, 10424, 105738, 98393, 121288, 3837, 110054, 115385, 98525, 132005, 99356, 94434, 14974, 1466, 1530, 1262, 1055, 46676, 4197, 339, 301, 13884, 8957, 19705, 1499, 55, 19850, 4142, 139050, 94746, 139050, 20333, 69, 2300, 339, 3362, 139050, 70, 343, 13, 139050, 15680, 113490, 99134, 94651, 105560, 250, 98409, 122269, 126666, 98372, 24659, 96, 98694, 12268, 246, 107006, 98662, 108808, 74516, 255, 135544, 3837, 98319, 123979, 125931, 107849, 98372, 125931, 98569, 131359, 98347, 98742, 109434, 3837, 110780, 98372, 125931, 25131, 231, 113001, 109611, 98670, 131513, 98437, 105556, 2045, 107693, 3837, 112032, 58, 60968, 13084, 251, 109470, 98670, 98579, 135544, 121288, 3837, 98501, 105954, 105954, 117545, 104915, 122918, 100090, 9904, 68494, 33, 2080, 268, 7552, 104594, 99424, 3837, 98501, 105954, 99093, 99502, 98670, 122055, 99754, 98317, 109470, 23621, 109, 98800, 53199, 99, 99052, 3837, 98333, 106996, 126309, 98314, 124406, 111908, 1773, 109470, 98530, 3837, 1112, 105305, 101255, 98705, 101252, 17, 17, 98874, 122806, 98366, 15350, 115, 98914, 111002, 3837, 98485, 26107, 122, 29653, 101, 98591, 98793, 82028, 112, 99009, 119523, 98753, 100232, 111234, 124553, 98412, 105656, 3837, 99927, 19053, 110, 99093, 98573, 98416, 105656, 117283, 16146, 94, 98784, 110031, 242, 126666, 137560, 98986, 113427, 104834, 112944, 99652, 3837, 119394, 111234, 98626, 137560, 133841, 130501, 98784, 82114, 102, 8078, 239, 98404, 104834, 98460, 116184, 3837, 100467, 100038, 98324, 99980, 108808, 24892, 98407, 106644, 3837, 98319, 132666, 115173, 131342, 148807, 98320, 118117, 100120, 98705, 98366, 3837, 52064, 252, 109886, 98416, 117283, 98626, 15588, 105, 132666, 147395, 99203, 115, 98679, 138813, 99456, 3837, 105560, 255, 134302, 132666, 115173, 52064, 252, 98364, 24273, 232, 99048, 1773, 1925, 477, 471, 454, 748, 142121, 19451, 336, 1068, 1055, 3927, 5503, 23269, 12979, 5477, 461, 6924, 3481, 704, 321, 4648, 94397, 1775, 2042, 6664, 10704, 1785, 21355, 43117, 11, 410, 300, 13470, 37114, 6664, 10704, 7873, 113222, 98799, 1162, 54, 2821, 60, 124553, 98412, 102484, 105560, 255, 15350, 228, 94651, 105288, 122269, 1341, 698, 1, 113623, 103703, 100774, 28213, 151337, 198, 102243, 101137, 3837, 104204, 2073, 99207, 101572, 99442, 105852, 102494, 117373, 98315, 99939, 854, 98314, 101839, 121524, 100338, 98549, 99334, 118032, 99160, 98802, 98319, 101733, 99974, 101191, 98992, 101839, 109254, 3837, 102756, 100338, 100837, 1773, 100889, 3837, 98711, 101839, 109254, 98547, 101390, 98353, 100395, 98323, 100338, 2073, 126544, 101067, 98483, 126779, 854, 98314, 102949, 1773, 99281, 3837, 99546, 99207, 101572, 99442, 98605, 103567, 120599, 103189, 117220, 98346, 101839, 100234, 98314, 112017, 1773, 102016, 3837, 105057, 102243, 105022, 101219, 99766, 99546, 99207, 99442, 105852, 120599, 98870, 3837, 99281, 3837, 98711, 103189, 98316, 110066, 1773, 151329]
chosen_inputs:
[gMASK] <sop> <|user|> 
你是一个事实核查领域解释生成的专家，擅长根据证据和真实性生成相应的解释。"
"我将提供当前说法、相应的证据以及当前说法真实性，你要根据证据和真实性，生成一段解释，说明当前说法为什么是对的/错的/证据不足或不充分的。"
"注意，你的解释要严格按照证据和真实性来。"
"当前说法为：美國高級夜總會一場美金一萬元"
"对应证据为：["证据1: 網傳「美國高級夜總會一場美金壹萬元，臺中忠信國小表演」的影片，經查證，流傳片段其實是北京的馬戲團參加法國電視節目時，表演雜技的片段，而非流傳內容所述來自台灣。主要流傳這段影片並在社群平台流傳：查證解釋：實際查看網傳內容中附上的YouTube連結，可以發現該影片其實是來自中國北京的馬戲團參加法國節目的表演片段，而非台灣的表演團體。另外，也可以在網路上找到其他類似的中國雜技表演節目。至於流傳內容所附上的YouTube影片，是由法國節目主持人PatrickSébastien主持的電視娛樂節目《LePlusGrandCabaretdumonde》於2011年時發布的片段，而相同影片也有英文標題版本〈StarsofBeijing'sCircus-Acrobaticact-TheworldgreatestCabaret〉。根據下方資訊欄的簡介說明，這是由來自北京的馬戲團所帶來的雜技表演，並非網傳所述是「臺中市忠信國小」的演出。此外，透過關鍵字搜尋，也能找到來自中國且相似的雜技表演「大蹬人」。因此，根據查證，網傳描述是錯誤的。面臨親人即將過世，是件很煎熬也很不容易的事，不捨激動是免不了的心理反應，悲傷難過也是正常的情緒表達。Search&FindAmazingSingles.Safe,SecureDatingforMatureSingles.BuildConnections&FindLove.ChatNow!吃巧克力總是讓人有幸福感，目前越來越多研究都發現，吃巧克力的好處有益於心血管，且帶給您滿滿好心情！近年受到新冠疫情影響，市場資料顯示，購買美妝保養產品，隨著疫情從線下轉到線上，在網路電商上有大幅的成長，其中有79.2%因為長戴口罩，所以更加重視臉部保養房屋外牆使用久了出現損壞的話，可能會有漏水的問題發生，導致房屋的內部也開始滲水，影響到日常生活需要定期修補Whatdoestheofficeofane-commercegiantlooklike?FindoutviaaquicktourofLazadaOne,sprawlingsixfloorsforworkandfun–allinone!不論是開早餐店、小吃攤、或是熱炒店，烹煮食物總會產生許多油煙，若沒好好處理，不但會遭到鄰居投訴，吸入過多的油煙對自己的身體健康也會造成影響（中央社記者趙靜瑜台北28日電）為了表彰社造工作者、民","证据2: 網傳「美國高級夜總會一場美金壹萬元，臺中忠信國小表演」的影片，經查證，流傳片段其實是北京的馬戲團參加法國電視節目時，表演雜技的片段，而非流傳內容所述來自台灣。主要流傳這段影片並在社群平台流傳：查證解釋：實際查看網傳內容中附上的YouTube連結，可以發現該影片其實是來自中國北京的馬戲團參加法國節目的表演片段，而非台灣的表演團體。另外，也可以在網路上找到其他類似的中國雜技表演節目。至於流傳內容所附上的YouTube影片，是由法國節目主持人PatrickSébastien主持的電視娛樂節目《LePlusGrandCabaretdumonde》於2011年時發布的片段，而相同影片也有英文標題版本〈StarsofBeijing'sCircus-Acrobaticact-TheworldgreatestCabaret〉。根據下方資訊欄的簡介說明，這是由來自北京的馬戲團所帶來的雜技表演，並非網傳所述是「臺中市忠信國小」的演出。此外，透過關鍵字搜尋，也能找到來自中國且相似的雜技表演「大蹬人」。因此，根據查證，網傳描述是錯誤的。記者黃俊昇／台中報導修平科技大學數媒系二十七日舉辦第十五屆畢業成果展，以「雙聲戲院」為主題，一共展出十組作品[…]PowerahousefulofdeviceswiththelatestbreakthroughfromXfinity– WiFi speedfasterthana gig. ​社群中心／綜合報導公廣集團再度引發爭議，在華視與公視接連出包後，昨晚公視臉書一篇台語教學PO文中，疑似[...]針對台海議題，美國國務卿布林肯（AnthonyBlinken）日前表示，美國政府支持台灣建立不對稱防禦能力，以阻止中國的武力侵略。對此，...本土昨增8822例創下歷史新高，民眾黨立委賴香伶今召開記者會，呼籲政府各部會應盡快輔導企業落實防疫長制度，並開放企業購買快篩劑等防疫物資，避免疫情大爆發；同時，在職場確診人數暴增下，勞動部應放寬職災傷病給付，維護職場勞工權益。Onestartup’sinnovativemethodofrestoringnutrient-strippedtopsoilhasmajorglobalimplications.Betterstill,ithasstock-priceimplications.[周刊王CTWANT]記者蔡維歆／台北報"]"
"请你生成解释：
 <|assistant|> 
证据表明，所谓的“美国高级夜总会一场美金一万元”的表演实际上是来自中国北京的马戏团在法国电视节目中的表演片段，并非来自台湾。此外，该表演片段被错误地描述为来自“台中忠信国小”的演出。因此，关于美国高级夜总会的费用的说法是基于对表演来源的误解。实际上，提供的证据并未涉及任何关于美国夜总会费用的信息，因此，该说法是错误的。 <|endoftext|>
chosen_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 102243, 101137, 3837, 104204, 2073, 99207, 101572, 99442, 105852, 102494, 117373, 98315, 99939, 854, 98314, 101839, 121524, 100338, 98549, 99334, 118032, 99160, 98802, 98319, 101733, 99974, 101191, 98992, 101839, 109254, 3837, 102756, 100338, 100837, 1773, 100889, 3837, 98711, 101839, 109254, 98547, 101390, 98353, 100395, 98323, 100338, 2073, 126544, 101067, 98483, 126779, 854, 98314, 102949, 1773, 99281, 3837, 99546, 99207, 101572, 99442, 98605, 103567, 120599, 103189, 117220, 98346, 101839, 100234, 98314, 112017, 1773, 102016, 3837, 105057, 102243, 105022, 101219, 99766, 99546, 99207, 99442, 105852, 120599, 98870, 3837, 99281, 3837, 98711, 103189, 98316, 110066, 1773, 151329]
chosen_labels:

证据表明，所谓的“美国高级夜总会一场美金一万元”的表演实际上是来自中国北京的马戏团在法国电视节目中的表演片段，并非来自台湾。此外，该表演片段被错误地描述为来自“台中忠信国小”的演出。因此，关于美国高级夜总会的费用的说法是基于对表演来源的误解。实际上，提供的证据并未涉及任何关于美国夜总会费用的信息，因此，该说法是错误的。 <|endoftext|>
rejected_input_ids:
[151331, 151333, 151336, 198, 98406, 99950, 100867, 111434, 99788, 100774, 120950, 100153, 3837, 106300, 99245, 102243, 98327, 117827, 103703, 102676, 100774, 1773, 698, 1, 113440, 99089, 100968, 103189, 5373, 102676, 102243, 99073, 100968, 103189, 117827, 3837, 103517, 99245, 102243, 98327, 117827, 3837, 103703, 101688, 100774, 3837, 100205, 100968, 103189, 99921, 125900, 14, 113068, 14, 102243, 100767, 109659, 110122, 1773, 698, 1, 99654, 3837, 99444, 100774, 98339, 111830, 102243, 98327, 117827, 98341, 1773, 698, 1, 100968, 103189, 98323, 5122, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 98315, 125319, 98575, 698, 1, 102917, 102243, 98323, 5122, 1183, 102243, 16, 25, 10231, 114, 110, 119851, 12874, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 109229, 125319, 98575, 3837, 25131, 118, 98322, 101067, 98483, 105954, 98380, 101839, 10424, 98314, 103973, 3837, 110653, 98790, 90289, 231, 3837, 98596, 119851, 109254, 98413, 113427, 98316, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 114446, 125931, 127255, 98468, 106644, 3837, 101839, 24090, 250, 98544, 98314, 109254, 3837, 107912, 98596, 119851, 115447, 98658, 105764, 107388, 98356, 98670, 122055, 1773, 98924, 98596, 119851, 106679, 98847, 103973, 119394, 98319, 113490, 99381, 98596, 119851, 5122, 98790, 90289, 231, 98539, 13084, 233, 5122, 113427, 131599, 102866, 120347, 119851, 115447, 98658, 98322, 99627, 99184, 53389, 131359, 124001, 3837, 98516, 108808, 111307, 49103, 110, 103973, 98413, 113427, 98316, 107388, 98356, 126309, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 127255, 100150, 101839, 109254, 3837, 107912, 98670, 122055, 98314, 101839, 12268, 246, 112237, 1773, 100470, 3837, 100785, 98319, 120347, 102335, 100813, 99148, 134479, 99362, 103294, 105954, 24090, 250, 98544, 101839, 127255, 98468, 1773, 98634, 105189, 98596, 119851, 115447, 98658, 98415, 99627, 99184, 53389, 103973, 3837, 102047, 98378, 105954, 127255, 98468, 105448, 64744, 50, 963, 27916, 3591, 101339, 98314, 114446, 125931, 98828, 249, 121338, 127255, 98468, 26852, 2304, 21755, 39930, 97371, 546, 1296, 372, 17211, 24991, 105189, 98546, 16, 98334, 106644, 108808, 98771, 98314, 109254, 3837, 98396, 101457, 103973, 100051, 103577, 144770, 121288, 102474, 107778, 12693, 704, 69, 3430, 23584, 594, 34, 2437, 355, 6691, 66, 22679, 774, 531, 70000, 94967, 70, 11170, 97371, 11297, 107498, 1773, 98836, 99099, 248, 105367, 116184, 28048, 232, 24273, 226, 98314, 138548, 99029, 113395, 98478, 3837, 106679, 102047, 107388, 98356, 121828, 144140, 7536, 110, 12268, 246, 98415, 13503, 114, 107388, 98314, 24090, 250, 98544, 101839, 3837, 119394, 98686, 120347, 119851, 105764, 98316, 12874, 25131, 118, 116718, 101067, 98483, 105954, 98380, 10424, 98314, 102949, 1773, 100889, 3837, 99765, 110511, 113911, 165, 235, 113, 98845, 100750, 15786, 233, 3837, 102057, 100813, 107388, 98356, 126309, 98833, 121281, 24090, 250, 98544, 101839, 12874, 98324, 125343, 98320, 107029, 99281, 3837, 98836, 99099, 248, 98790, 90289, 231, 3837, 120347, 119851, 100395, 98316, 89928, 107, 44557, 97, 98314, 1773, 98389, 25131, 101, 139619, 98320, 98837, 114811, 110511, 98647, 3837, 98316, 98578, 98443, 123152, 102281, 105150, 99508, 3837, 98317, 14665, 101, 99220, 109886, 98316, 99162, 101044, 107711, 98759, 117283, 3837, 101015, 99203, 115, 136584, 110511, 99002, 106454, 98421, 147267, 98470, 135700, 1773, 5890, 5, 9880, 64598, 50, 49285, 89086, 11, 49495, 64654, 1958, 44, 1568, 50, 49285, 25134, 54385, 5, 9880, 28154, 58437, 7039, 0, 98948, 110042, 114143, 121, 98316, 121924, 111817, 111295, 3837, 99332, 98813, 107388, 107507, 98743, 98401, 108808, 111307, 3837, 98948, 100241, 98819, 100583, 98384, 124834, 108081, 105189, 110604, 3837, 98833, 13503, 114, 138813, 99526, 162, 3219, 162, 3219, 98384, 102823, 6313, 112259, 100049, 118156, 98603, 143033, 3837, 145642, 116184, 98617, 13837, 107, 98671, 3837, 133841, 130501, 98501, 98355, 251, 98502, 148800, 119594, 98433, 3837, 36504, 101, 99423, 100038, 117695, 133545, 98366, 15201, 231, 98344, 133545, 98332, 3837, 98319, 120347, 98550, 114446, 98650, 104111, 103178, 98314, 98350, 112944, 3837, 112619, 102626, 13, 17, 4, 98453, 105738, 112944, 123456, 3837, 98949, 99835, 98429, 125931, 25131, 231, 98416, 98502, 148800, 102191, 98481, 16616, 228, 98991, 110614, 98347, 111307, 98911, 235, 44467, 252, 98314, 126665, 3837, 98896, 105656, 98318, 101400, 103514, 136713, 108808, 98330, 3837, 126666, 98908, 102191, 98314, 115447, 98416, 98361, 111234, 98788, 71941, 110, 98442, 3837, 98603, 143033, 98344, 109224, 98862, 102785, 99007, 141509, 3838, 2982, 477, 383, 26431, 1055, 2145, 41350, 70, 5372, 7201, 4803, 30, 9880, 411, 20065, 64, 27669, 83, 49508, 69, 43, 1370, 2584, 3966, 10666, 137505, 18779, 941, 89064, 1087, 1958, 1778, 437, 11888, 4142, 541, 258, 603, 0, 98317, 118421, 98316, 111234, 105735, 99090, 5373, 109652, 13522, 97, 5373, 102821, 142152, 100332, 99090, 3837, 105904, 101005, 101094, 114143, 121, 105656, 119594, 98330, 28048, 109, 98362, 98882, 98680, 247, 3837, 99533, 118221, 102156, 124834, 98367, 3837, 103636, 105656, 106695, 103396, 108, 99109, 98810, 28048, 112, 3837, 115193, 110511, 100499, 98882, 98680, 247, 109470, 98983, 98542, 112237, 99370, 98361, 105656, 99896, 98603, 143033, 9904, 100149, 98551, 124553, 98412, 48852, 247, 20965, 250, 105304, 105288, 99869, 98385, 114446, 7552, 105738, 98321, 106660, 98551, 98663, 104557, 5373, 98485, 2198, 102243, 17, 25, 10231, 114, 110, 119851, 12874, 98501, 105954, 98377, 140790, 99442, 114143, 121, 105656, 98315, 115173, 117373, 109229, 125319, 98575, 3837, 25131, 118, 98322, 101067, 98483, 105954, 98380, 101839, 10424, 98314, 103973, 3837, 110653, 98790, 90289, 231, 3837, 98596, 119851, 109254, 98413, 113427, 98316, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 114446, 125931, 127255, 98468, 106644, 3837, 101839, 24090, 250, 98544, 98314, 109254, 3837, 107912, 98596, 119851, 115447, 98658, 105764, 107388, 98356, 98670, 122055, 1773, 98924, 98596, 119851, 106679, 98847, 103973, 119394, 98319, 113490, 99381, 98596, 119851, 5122, 98790, 90289, 231, 98539, 13084, 233, 5122, 113427, 131599, 102866, 120347, 119851, 115447, 98658, 98322, 99627, 99184, 53389, 131359, 124001, 3837, 98516, 108808, 111307, 49103, 110, 103973, 98413, 113427, 98316, 107388, 98356, 126309, 121828, 144140, 7536, 110, 12268, 246, 5758, 225, 98431, 98378, 105954, 127255, 100150, 101839, 109254, 3837, 107912, 98670, 122055, 98314, 101839, 12268, 246, 112237, 1773, 100470, 3837, 100785, 98319, 120347, 102335, 100813, 99148, 134479, 99362, 103294, 105954, 24090, 250, 98544, 101839, 127255, 98468, 1773, 98634, 105189, 98596, 119851, 115447, 98658, 98415, 99627, 99184, 53389, 103973, 3837, 102047, 98378, 105954, 127255, 98468, 105448, 64744, 50, 963, 27916, 3591, 101339, 98314, 114446, 125931, 98828, 249, 121338, 127255, 98468, 26852, 2304, 21755, 39930, 97371, 546, 1296, 372, 17211, 24991, 105189, 98546, 16, 98334, 106644, 108808, 98771, 98314, 109254, 3837, 98396, 101457, 103973, 100051, 103577, 144770, 121288, 102474, 107778, 12693, 704, 69, 3430, 23584, 594, 34, 2437, 355, 6691, 66, 22679, 774, 531, 70000, 94967, 70, 11170, 97371, 11297, 107498, 1773, 98836, 99099, 248, 105367, 116184, 28048, 232, 24273, 226, 98314, 138548, 99029, 113395, 98478, 3837, 106679, 102047, 107388, 98356, 121828, 144140, 7536, 110, 12268, 246, 98415, 13503, 114, 107388, 98314, 24090, 250, 98544, 101839, 3837, 119394, 98686, 120347, 119851, 105764, 98316, 12874, 25131, 118, 116718, 101067, 98483, 105954, 98380, 10424, 98314, 102949, 1773, 100889, 3837, 99765, 110511, 113911, 165, 235, 113, 98845, 100750, 15786, 233, 3837, 102057, 100813, 107388, 98356, 126309, 98833, 121281, 24090, 250, 98544, 101839, 12874, 98324, 125343, 98320, 107029, 99281, 3837, 98836, 99099, 248, 98790, 90289, 231, 3837, 120347, 119851, 100395, 98316, 89928, 107, 44557, 97, 98314, 1773, 124553, 98412, 29653, 225, 102018, 11615, 229, 94651, 126544, 122269, 126666, 99007, 98492, 99296, 98324, 105556, 118117, 99599, 98473, 100053, 99211, 98385, 61570, 231, 15815, 99, 126247, 22701, 228, 38887, 95, 110318, 100652, 98447, 3837, 98333, 12874, 24090, 247, 35457, 110, 7536, 110, 98585, 10424, 105738, 98393, 121288, 3837, 110054, 115385, 98525, 132005, 99356, 94434, 14974, 1466, 1530, 1262, 1055, 46676, 4197, 339, 301, 13884, 8957, 19705, 1499, 55, 19850, 4142, 139050, 94746, 139050, 20333, 69, 2300, 339, 3362, 139050, 70, 343, 13, 139050, 15680, 113490, 99134, 94651, 105560, 250, 98409, 122269, 126666, 98372, 24659, 96, 98694, 12268, 246, 107006, 98662, 108808, 74516, 255, 135544, 3837, 98319, 123979, 125931, 107849, 98372, 125931, 98569, 131359, 98347, 98742, 109434, 3837, 110780, 98372, 125931, 25131, 231, 113001, 109611, 98670, 131513, 98437, 105556, 2045, 107693, 3837, 112032, 58, 60968, 13084, 251, 109470, 98670, 98579, 135544, 121288, 3837, 98501, 105954, 105954, 117545, 104915, 122918, 100090, 9904, 68494, 33, 2080, 268, 7552, 104594, 99424, 3837, 98501, 105954, 99093, 99502, 98670, 122055, 99754, 98317, 109470, 23621, 109, 98800, 53199, 99, 99052, 3837, 98333, 106996, 126309, 98314, 124406, 111908, 1773, 109470, 98530, 3837, 1112, 105305, 101255, 98705, 101252, 17, 17, 98874, 122806, 98366, 15350, 115, 98914, 111002, 3837, 98485, 26107, 122, 29653, 101, 98591, 98793, 82028, 112, 99009, 119523, 98753, 100232, 111234, 124553, 98412, 105656, 3837, 99927, 19053, 110, 99093, 98573, 98416, 105656, 117283, 16146, 94, 98784, 110031, 242, 126666, 137560, 98986, 113427, 104834, 112944, 99652, 3837, 119394, 111234, 98626, 137560, 133841, 130501, 98784, 82114, 102, 8078, 239, 98404, 104834, 98460, 116184, 3837, 100467, 100038, 98324, 99980, 108808, 24892, 98407, 106644, 3837, 98319, 132666, 115173, 131342, 148807, 98320, 118117, 100120, 98705, 98366, 3837, 52064, 252, 109886, 98416, 117283, 98626, 15588, 105, 132666, 147395, 99203, 115, 98679, 138813, 99456, 3837, 105560, 255, 134302, 132666, 115173, 52064, 252, 98364, 24273, 232, 99048, 1773, 1925, 477, 471, 454, 748, 142121, 19451, 336, 1068, 1055, 3927, 5503, 23269, 12979, 5477, 461, 6924, 3481, 704, 321, 4648, 94397, 1775, 2042, 6664, 10704, 1785, 21355, 43117, 11, 410, 300, 13470, 37114, 6664, 10704, 7873, 113222, 98799, 1162, 54, 2821, 60, 124553, 98412, 102484, 105560, 255, 15350, 228, 94651, 105288, 122269, 1341, 698, 1, 113623, 103703, 100774, 28213, 151337, 198, 102243, 100417, 101105, 3837, 107036, 98314, 103973, 121524, 100338, 98549, 99334, 118032, 7536, 110, 12268, 246, 98319, 101733, 99974, 101191, 98992, 101839, 3837, 107912, 100338, 100837, 98314, 126544, 101067, 98483, 105954, 98380, 1773, 99281, 3837, 98711, 103189, 98316, 110066, 1773, 151329]
rejected_inputs:
[gMASK] <sop> <|user|> 
你是一个事实核查领域解释生成的专家，擅长根据证据和真实性生成相应的解释。"
"我将提供当前说法、相应的证据以及当前说法真实性，你要根据证据和真实性，生成一段解释，说明当前说法为什么是对的/错的/证据不足或不充分的。"
"注意，你的解释要严格按照证据和真实性来。"
"当前说法为：美國高級夜總會一場美金一萬元"
"对应证据为：["证据1: 網傳「美國高級夜總會一場美金壹萬元，臺中忠信國小表演」的影片，經查證，流傳片段其實是北京的馬戲團參加法國電視節目時，表演雜技的片段，而非流傳內容所述來自台灣。主要流傳這段影片並在社群平台流傳：查證解釋：實際查看網傳內容中附上的YouTube連結，可以發現該影片其實是來自中國北京的馬戲團參加法國節目的表演片段，而非台灣的表演團體。另外，也可以在網路上找到其他類似的中國雜技表演節目。至於流傳內容所附上的YouTube影片，是由法國節目主持人PatrickSébastien主持的電視娛樂節目《LePlusGrandCabaretdumonde》於2011年時發布的片段，而相同影片也有英文標題版本〈StarsofBeijing'sCircus-Acrobaticact-TheworldgreatestCabaret〉。根據下方資訊欄的簡介說明，這是由來自北京的馬戲團所帶來的雜技表演，並非網傳所述是「臺中市忠信國小」的演出。此外，透過關鍵字搜尋，也能找到來自中國且相似的雜技表演「大蹬人」。因此，根據查證，網傳描述是錯誤的。面臨親人即將過世，是件很煎熬也很不容易的事，不捨激動是免不了的心理反應，悲傷難過也是正常的情緒表達。Search&FindAmazingSingles.Safe,SecureDatingforMatureSingles.BuildConnections&FindLove.ChatNow!吃巧克力總是讓人有幸福感，目前越來越多研究都發現，吃巧克力的好處有益於心血管，且帶給您滿滿好心情！近年受到新冠疫情影響，市場資料顯示，購買美妝保養產品，隨著疫情從線下轉到線上，在網路電商上有大幅的成長，其中有79.2%因為長戴口罩，所以更加重視臉部保養房屋外牆使用久了出現損壞的話，可能會有漏水的問題發生，導致房屋的內部也開始滲水，影響到日常生活需要定期修補Whatdoestheofficeofane-commercegiantlooklike?FindoutviaaquicktourofLazadaOne,sprawlingsixfloorsforworkandfun–allinone!不論是開早餐店、小吃攤、或是熱炒店，烹煮食物總會產生許多油煙，若沒好好處理，不但會遭到鄰居投訴，吸入過多的油煙對自己的身體健康也會造成影響（中央社記者趙靜瑜台北28日電）為了表彰社造工作者、民","证据2: 網傳「美國高級夜總會一場美金壹萬元，臺中忠信國小表演」的影片，經查證，流傳片段其實是北京的馬戲團參加法國電視節目時，表演雜技的片段，而非流傳內容所述來自台灣。主要流傳這段影片並在社群平台流傳：查證解釋：實際查看網傳內容中附上的YouTube連結，可以發現該影片其實是來自中國北京的馬戲團參加法國節目的表演片段，而非台灣的表演團體。另外，也可以在網路上找到其他類似的中國雜技表演節目。至於流傳內容所附上的YouTube影片，是由法國節目主持人PatrickSébastien主持的電視娛樂節目《LePlusGrandCabaretdumonde》於2011年時發布的片段，而相同影片也有英文標題版本〈StarsofBeijing'sCircus-Acrobaticact-TheworldgreatestCabaret〉。根據下方資訊欄的簡介說明，這是由來自北京的馬戲團所帶來的雜技表演，並非網傳所述是「臺中市忠信國小」的演出。此外，透過關鍵字搜尋，也能找到來自中國且相似的雜技表演「大蹬人」。因此，根據查證，網傳描述是錯誤的。記者黃俊昇／台中報導修平科技大學數媒系二十七日舉辦第十五屆畢業成果展，以「雙聲戲院」為主題，一共展出十組作品[…]PowerahousefulofdeviceswiththelatestbreakthroughfromXfinity– WiFi speedfasterthana gig. ​社群中心／綜合報導公廣集團再度引發爭議，在華視與公視接連出包後，昨晚公視臉書一篇台語教學PO文中，疑似[...]針對台海議題，美國國務卿布林肯（AnthonyBlinken）日前表示，美國政府支持台灣建立不對稱防禦能力，以阻止中國的武力侵略。對此，...本土昨增8822例創下歷史新高，民眾黨立委賴香伶今召開記者會，呼籲政府各部會應盡快輔導企業落實防疫長制度，並開放企業購買快篩劑等防疫物資，避免疫情大爆發；同時，在職場確診人數暴增下，勞動部應放寬職災傷病給付，維護職場勞工權益。Onestartup’sinnovativemethodofrestoringnutrient-strippedtopsoilhasmajorglobalimplications.Betterstill,ithasstock-priceimplications.[周刊王CTWANT]記者蔡維歆／台北報"]"
"请你生成解释：
 <|assistant|> 
证据明确指出，流传的影片实际上是来自中国北京的马戲團在法国电视节目中的表演，而非来自台湾的台中忠信國小。因此，该说法是错误的。 <|endoftext|>
rejected_label_ids:
[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 198, 102243, 100417, 101105, 3837, 107036, 98314, 103973, 121524, 100338, 98549, 99334, 118032, 7536, 110, 12268, 246, 98319, 101733, 99974, 101191, 98992, 101839, 3837, 107912, 100338, 100837, 98314, 126544, 101067, 98483, 105954, 98380, 1773, 99281, 3837, 98711, 103189, 98316, 110066, 1773, 151329]
rejected_labels:

证据明确指出，流传的影片实际上是来自中国北京的马戲團在法国电视节目中的表演，而非来自台湾的台中忠信國小。因此，该说法是错误的。 <|endoftext|>
[INFO|configuration_utils.py:731] 2024-08-11 12:05:28,705 >> loading configuration file /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft/config.json
[INFO|configuration_utils.py:731] 2024-08-11 12:05:28,711 >> loading configuration file /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft/config.json
[INFO|configuration_utils.py:796] 2024-08-11 12:05:28,712 >> Model config ChatGLMConfig {
  "_name_or_path": "/mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|modeling_utils.py:3471] 2024-08-11 12:05:28,756 >> loading weights file /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft/model.safetensors.index.json
[INFO|modeling_utils.py:1519] 2024-08-11 12:05:28,756 >> Instantiating ChatGLMForConditionalGeneration model under default dtype torch.bfloat16.
[INFO|configuration_utils.py:962] 2024-08-11 12:05:28,757 >> Generate config GenerationConfig {
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "pad_token_id": 151329
}

Loading checkpoint shards: 100%|█████████████████████████████████████| 10/10 [01:01<00:00,  6.20s/it]
08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
08/11/2024 12:06:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
08/11/2024 12:06:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: query_key_value,dense_h_to_4h,dense,dense_4h_to_h
Loading checkpoint shards: 100%|█████████████████████████████████████| 10/10 [01:03<00:00,  6.34s/it]
[INFO|modeling_utils.py:4280] 2024-08-11 12:06:32,202 >> All model checkpoint weights were used when initializing ChatGLMForConditionalGeneration.

[INFO|modeling_utils.py:4288] 2024-08-11 12:06:32,202 >> All the weights of ChatGLMForConditionalGeneration were initialized from the model checkpoint at /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft.
If your task is similar to the task the model of the checkpoint was trained on, you can already use ChatGLMForConditionalGeneration for predictions without further training.
[INFO|configuration_utils.py:915] 2024-08-11 12:06:32,206 >> loading configuration file /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft/generation_config.json
[INFO|configuration_utils.py:962] 2024-08-11 12:06:32,206 >> Generate config GenerationConfig {
  "do_sample": true,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "max_length": 128000,
  "pad_token_id": 151329,
  "temperature": 0.8,
  "top_p": 0.8
}

08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.
08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.
08/11/2024 12:06:32 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.
08/11/2024 12:06:32 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA
08/11/2024 12:06:32 - INFO - llamafactory.model.model_utils.misc - Found linear modules: dense,dense_h_to_4h,dense_4h_to_h,query_key_value
08/11/2024 12:06:32 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
08/11/2024 12:06:32 - INFO - llamafactory.model.loader - trainable params: 21,176,320 || all params: 9,421,127,680 || trainable%: 0.2248
Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
[INFO|trainer.py:641] 2024-08-11 12:06:32,544 >> Using auto half precision backend
[INFO|trainer.py:2078] 2024-08-11 12:06:33,220 >> ***** Running training *****
[INFO|trainer.py:2079] 2024-08-11 12:06:33,220 >>   Num examples = 630
[INFO|trainer.py:2080] 2024-08-11 12:06:33,220 >>   Num Epochs = 3
[INFO|trainer.py:2081] 2024-08-11 12:06:33,220 >>   Instantaneous batch size per device = 1
[INFO|trainer.py:2084] 2024-08-11 12:06:33,220 >>   Total train batch size (w. parallel, distributed & accumulation) = 16
[INFO|trainer.py:2085] 2024-08-11 12:06:33,220 >>   Gradient Accumulation steps = 8
[INFO|trainer.py:2086] 2024-08-11 12:06:33,220 >>   Total optimization steps = 117
[INFO|trainer.py:2087] 2024-08-11 12:06:33,223 >>   Number of trainable parameters = 21,176,320
{'loss': 0.6931, 'grad_norm': 8.871271133422852, 'learning_rate': 4.166666666666667e-06, 'rewards/chosen': 0.003708322299644351, 'rewards/rejected': -0.00020000133372377604, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': 0.003908323589712381, 'logps/rejected': -37.936092376708984, 'logps/chosen': -60.76411819458008, 'logits/rejected': -0.9438970685005188, 'logits/chosen': -0.9468690156936646, 'epoch': 0.25}
{'loss': 0.6632, 'grad_norm': 8.393320083618164, 'learning_rate': 4.9287250957321685e-06, 'rewards/chosen': -0.027964288368821144, 'rewards/rejected': -0.08705427497625351, 'rewards/accuracies': 0.737500011920929, 'rewards/margins': 0.059089988470077515, 'logps/rejected': -35.69167709350586, 'logps/chosen': -59.1557502746582, 'logits/rejected': -0.9545572400093079, 'logits/chosen': -0.9552885890007019, 'epoch': 0.51}
{'loss': 0.5708, 'grad_norm': 8.12808609008789, 'learning_rate': 4.646121984004666e-06, 'rewards/chosen': -0.1379518359899521, 'rewards/rejected': -0.46064862608909607, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 0.3226968050003052, 'logps/rejected': -43.205177307128906, 'logps/chosen': -68.43035888671875, 'logits/rejected': -0.9448691606521606, 'logits/chosen': -0.9575769305229187, 'epoch': 0.76}
{'loss': 0.4792, 'grad_norm': 7.7298431396484375, 'learning_rate': 4.172826515897146e-06, 'rewards/chosen': -0.25774940848350525, 'rewards/rejected': -0.8361514806747437, 'rewards/accuracies': 0.875, 'rewards/margins': 0.5784021019935608, 'logps/rejected': -44.60936737060547, 'logps/chosen': -66.322021484375, 'logits/rejected': -0.9502445459365845, 'logits/chosen': -0.956710696220398, 'epoch': 1.02}
{'loss': 0.3784, 'grad_norm': 7.882524013519287, 'learning_rate': 3.5508930707739143e-06, 'rewards/chosen': -0.3377012610435486, 'rewards/rejected': -1.3774535655975342, 'rewards/accuracies': 0.9375, 'rewards/margins': 1.0397522449493408, 'logps/rejected': -48.568084716796875, 'logps/chosen': -64.79175567626953, 'logits/rejected': -0.9304746389389038, 'logits/chosen': -0.9365676641464233, 'epoch': 1.27}
{'loss': 0.3247, 'grad_norm': 8.662622451782227, 'learning_rate': 2.835583164544139e-06, 'rewards/chosen': -0.6295241117477417, 'rewards/rejected': -1.790339708328247, 'rewards/accuracies': 0.925000011920929, 'rewards/margins': 1.1608154773712158, 'logps/rejected': -54.03873825073242, 'logps/chosen': -68.06593322753906, 'logits/rejected': -0.9226783514022827, 'logits/chosen': -0.9274141192436218, 'epoch': 1.52}
{'loss': 0.2947, 'grad_norm': 7.34816837310791, 'learning_rate': 2.090455221462156e-06, 'rewards/chosen': -0.787818968296051, 'rewards/rejected': -2.539358615875244, 'rewards/accuracies': 0.8999999761581421, 'rewards/margins': 1.751539945602417, 'logps/rejected': -59.9039192199707, 'logps/chosen': -73.60018920898438, 'logits/rejected': -0.9287379384040833, 'logits/chosen': -0.9322299957275391, 'epoch': 1.78}
{'loss': 0.2713, 'grad_norm': 7.777676105499268, 'learning_rate': 1.3817171292109182e-06, 'rewards/chosen': -0.9496301412582397, 'rewards/rejected': -2.7478368282318115, 'rewards/accuracies': 0.949999988079071, 'rewards/margins': 1.7982066869735718, 'logps/rejected': -63.222442626953125, 'logps/chosen': -73.36286926269531, 'logits/rejected': -0.9498062133789062, 'logits/chosen': -0.9481372833251953, 'epoch': 2.03}
{'loss': 0.2438, 'grad_norm': 8.434239387512207, 'learning_rate': 7.723433775328385e-07, 'rewards/chosen': -1.1458582878112793, 'rewards/rejected': -3.3909783363342285, 'rewards/accuracies': 0.9125000238418579, 'rewards/margins': 2.2451207637786865, 'logps/rejected': -71.83565521240234, 'logps/chosen': -77.96927642822266, 'logits/rejected': -0.9191953539848328, 'logits/chosen': -0.9306413531303406, 'epoch': 2.29}
{'loss': 0.2387, 'grad_norm': 9.010306358337402, 'learning_rate': 3.164794984571759e-07, 'rewards/chosen': -1.0749781131744385, 'rewards/rejected': -3.0630455017089844, 'rewards/accuracies': 0.9624999761581421, 'rewards/margins': 1.9880670309066772, 'logps/rejected': -65.40312957763672, 'logps/chosen': -77.60609436035156, 'logits/rejected': -0.9391613006591797, 'logits/chosen': -0.9428731799125671, 'epoch': 2.54}
{'loss': 0.2381, 'grad_norm': 6.236480236053467, 'learning_rate': 5.463099816548578e-08, 'rewards/chosen': -1.0434600114822388, 'rewards/rejected': -3.212085008621216, 'rewards/accuracies': 0.949999988079071, 'rewards/margins': 2.1686251163482666, 'logps/rejected': -68.20579528808594, 'logps/chosen': -72.98116302490234, 'logits/rejected': -0.9278262257575989, 'logits/chosen': -0.9309724569320679, 'epoch': 2.79}
100%|██████████████████████████████████████████████████████████████| 117/117 [27:52<00:00, 13.82s/it][INFO|trainer.py:2329] 2024-08-11 12:34:25,810 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 1672.5869, 'train_samples_per_second': 1.13, 'train_steps_per_second': 0.07, 'train_loss': 0.3863630447632227, 'epoch': 2.97}
100%|██████████████████████████████████████████████████████████████| 117/117 [27:52<00:00, 14.29s/it]
[INFO|trainer.py:3410] 2024-08-11 12:34:25,814 >> Saving model checkpoint to saves/glm4_CHEF_dpo
[INFO|configuration_utils.py:731] 2024-08-11 12:34:25,837 >> loading configuration file /mnt/user/luyifei/model_weight/glm4_CHEF_lora_sft/config.json
[INFO|configuration_utils.py:796] 2024-08-11 12:34:25,838 >> Model config ChatGLMConfig {
  "_name_or_path": "/mnt/user/luyifei/model_weight/glm4-9b-chat/glm-4-9b-chat",
  "add_bias_linear": false,
  "add_qkv_bias": true,
  "apply_query_key_layer_scaling": true,
  "apply_residual_connection_post_layernorm": false,
  "architectures": [
    "ChatGLMForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "attention_softmax_in_fp32": true,
  "auto_map": {
    "AutoConfig": "configuration_chatglm.ChatGLMConfig",
    "AutoModel": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForCausalLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSeq2SeqLM": "modeling_chatglm.ChatGLMForConditionalGeneration",
    "AutoModelForSequenceClassification": "modeling_chatglm.ChatGLMForSequenceClassification"
  },
  "bias_dropout_fusion": true,
  "classifier_dropout": null,
  "eos_token_id": [
    151329,
    151336,
    151338
  ],
  "ffn_hidden_size": 13696,
  "fp32_residual_connection": false,
  "hidden_dropout": 0.0,
  "hidden_size": 4096,
  "kv_channels": 128,
  "layernorm_epsilon": 1.5625e-07,
  "model_type": "chatglm",
  "multi_query_attention": true,
  "multi_query_group_num": 2,
  "num_attention_heads": 32,
  "num_hidden_layers": 40,
  "num_layers": 40,
  "original_rope": true,
  "pad_token_id": 151329,
  "padded_vocab_size": 151552,
  "post_layer_norm": true,
  "rmsnorm": true,
  "rope_ratio": 500,
  "seq_length": 131072,
  "tie_word_embeddings": false,
  "torch_dtype": "bfloat16",
  "transformers_version": "4.41.2",
  "use_cache": true,
  "vocab_size": 151552
}

[INFO|tokenization_utils_base.py:2513] 2024-08-11 12:34:25,980 >> tokenizer config file saved in saves/glm4_CHEF_dpo/tokenizer_config.json
[INFO|tokenization_utils_base.py:2522] 2024-08-11 12:34:25,981 >> Special tokens file saved in saves/glm4_CHEF_dpo/special_tokens_map.json
[INFO|tokenization_utils_base.py:2573] 2024-08-11 12:34:25,983 >> added tokens file saved in saves/glm4_CHEF_dpo/added_tokens.json
***** train metrics *****
  epoch                    =      2.9714
  total_flos               = 263644212GF
  train_loss               =      0.3864
  train_runtime            =  0:27:52.58
  train_samples_per_second =        1.13
  train_steps_per_second   =        0.07
Figure saved at: saves/glm4_CHEF_dpo/training_loss.png
08/11/2024 12:34:26 - WARNING - llamafactory.extras.ploting - No metric eval_loss to plot.
Figure saved at: saves/glm4_CHEF_dpo/training_rewards_accuracies.png
[INFO|trainer.py:3719] 2024-08-11 12:34:26,179 >> ***** Running Evaluation *****
[INFO|trainer.py:3721] 2024-08-11 12:34:26,180 >>   Num examples = 70
[INFO|trainer.py:3724] 2024-08-11 12:34:26,180 >>   Batch size = 1
100%|████████████████████████████████████████████████████████████████| 35/35 [00:29<00:00,  1.19it/s]
***** eval metrics *****
  epoch                   =     2.9714
  eval_logits/chosen      =    -0.9351
  eval_logits/rejected    =     -0.922
  eval_logps/chosen       =   -79.2636
  eval_logps/rejected     =   -73.1585
  eval_loss               =     0.2942
  eval_rewards/accuracies =     0.9429
  eval_rewards/chosen     =    -1.3571
  eval_rewards/margins    =     2.2219
  eval_rewards/rejected   =     -3.579
  eval_runtime            = 0:00:30.15
  eval_samples_per_second =      2.321
  eval_steps_per_second   =      1.161
[INFO|modelcard.py:450] 2024-08-11 12:34:56,342 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}